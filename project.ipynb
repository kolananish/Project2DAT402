{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ceba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"gpiosenka/cards-image-datasetclassification\")\n",
    "\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3c8d6",
   "metadata": {},
   "source": [
    "# Problem Definition & Motivation\n",
    "\n",
    "We are using the dataset \"Cards Image Dataset-Classification\" with 7624 train images, 265 test images, and 265 validation images. \n",
    "https://www.kaggle.com/datasets/gpiosenka/cards-image-datasetclassification \n",
    "\n",
    "**Goal**\n",
    "Build a model that, given a photo of a playing card, predicts which card it is (e.g. “ace of hearts”, “king of spades”, etc.).\n",
    "\n",
    "**Why it’s interesting**:\n",
    "- Real-world vision challenge: cards vary in orientation, lighting, occlusion, background clutter.\n",
    "- Hierarchical structure: each card has a rank (A, 2…K) and a suit (♥, ♣, ♦, ♠).\n",
    "- Can be framed as a flat 53-way classification or a multi‐output (rank + suit) problem.\n",
    "\n",
    "**Dataset**:\n",
    "- Stored in `train/`, `valid/`, `test/` subfolders, one folder per class (e.g. `ace of hearts`, `two of spades`, …, `joker`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa6cdf",
   "metadata": {},
   "source": [
    "We first want to load our dataset into our project. We can see the train, validation, and test folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c6cfa",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and dataset path\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust this path to wherever you unpacked the dataset\n",
    "DATA_ROOT = Path('/home/vscode/.cache/kagglehub/datasets/gpiosenka/cards-image-datasetclassification/versions/2')\n",
    "\n",
    "train_dir = DATA_ROOT / 'train'\n",
    "valid_dir = DATA_ROOT / 'valid'\n",
    "test_dir  = DATA_ROOT / 'test'\n",
    "\n",
    "print(\"Train folder:\", train_dir.resolve())\n",
    "print(\"Valid folder:\", valid_dir.resolve())\n",
    "print(\"Test  folder:\", test_dir.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788fa74",
   "metadata": {},
   "source": [
    "Now we want to view all the different types of cards we have to get an understanding of the types of cards within each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497959b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique classes in the training folder\n",
    "train_classes = [folder.name for folder in train_dir.iterdir()\n",
    "                 if folder.is_dir()]\n",
    "print(f\"Unique classes in the training folder ({len(train_classes)}):\")\n",
    "for card_class in train_classes:\n",
    "    print(card_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139473d",
   "metadata": {},
   "source": [
    "We noticed that each class type has a folder with a space in the text. For example, \"ace of spades\" has two spaces. When converting this dataset into our own dataframe, these spaces cause issues. \n",
    "\n",
    "Our next preocessing step is to replace each of the spaces of the folders with underscores to easily access the images with no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update folder names to replace spaces with underscores\n",
    "\n",
    "def update_folder_names(directory):\n",
    "    for folder in os.listdir(directory):\n",
    "        if ' ' in folder:\n",
    "            old_path = os.path.join(directory, folder)\n",
    "            new_folder_name = folder.replace(' ', '_')\n",
    "            new_path = os.path.join(directory, new_folder_name)\n",
    "            os.rename(old_path, new_path)\n",
    "            # print(f\"Renamed folder: {folder} → {new_folder_name}\")\n",
    "\n",
    "\n",
    "# Apply the function to train, valid, and test directories\n",
    "update_folder_names(train_dir)\n",
    "update_folder_names(valid_dir)\n",
    "update_folder_names(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b108fbb",
   "metadata": {},
   "source": [
    "For our pre-processing, we aim to use the traditional deck of cards (Diamond, Club, Spade, Hearts, and cards numbers from 2-10, Jack, Queen, King, Ace)\n",
    "\n",
    "Therefore, we will drop the Joker class to focus our machine learning model methods.\n",
    "\n",
    "In a real world environment, this model would help with individuals in the casino/gambling industry for identifying cards efficiently.\n",
    "\n",
    "We will first create a dataframe with the file paths from before, and then remove the Joker class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a DataFrame with file paths and labels\n",
    "def create_dataframe(directory):\n",
    "    data = []\n",
    "    for folder in directory.iterdir():\n",
    "        if folder.is_dir():\n",
    "            label = folder.name\n",
    "            # Assuming images are in JPG format\n",
    "            for file in folder.glob(\"*.jpg\"):\n",
    "                data.append({\"file_path\": str(file), \"label\": label})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Load the dataset into DataFrames\n",
    "train_df = create_dataframe(train_dir)\n",
    "valid_df = create_dataframe(valid_dir)\n",
    "test_df = create_dataframe(test_dir)\n",
    "\n",
    "# Remove the Joker class from the DataFrames\n",
    "train_df = train_df[train_df[\"label\"] != \"joker\"]\n",
    "valid_df = valid_df[valid_df[\"label\"] != \"joker\"]\n",
    "test_df = test_df[test_df[\"label\"] != \"joker\"]\n",
    "\n",
    "\n",
    "# Display the first few rows of the training DataFrame\n",
    "print(\"Training DataFrame:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Display unique classes in train_df\n",
    "unique_classes = train_df[\"label\"].unique()\n",
    "print(f\"Unique classes in train_df ({len(unique_classes)}):\")\n",
    "for card_class in unique_classes:\n",
    "    print(card_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e141ea1",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa9f1b",
   "metadata": {},
   "source": [
    "We will now view the train/test/validation split with the dataframe and display it in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167bbb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print train/test/validation split metadata\n",
    "def print_train_test_valid_amounts(train_data, test_data, valid_data):\n",
    "    print(\"\\nThe Train/Test/Validation Split:\\n--------------------------------\")\n",
    "    train_count = len(train_data)\n",
    "    print(\"# of Train Images:\", train_count)\n",
    "\n",
    "    test_count = len(test_data)\n",
    "    print(\"# of Test Images:\", test_count)\n",
    "\n",
    "    valid_count = len(valid_data)\n",
    "    print(\"# of Validation Images:\", valid_count)\n",
    "\n",
    "    total_count = train_count + test_count + valid_count\n",
    "    train_ratio = np.round(train_count / total_count, 2)\n",
    "    test_ratio = np.round(test_count / total_count, 2)\n",
    "    valid_ratio = np.round(valid_count / total_count, 2)\n",
    "\n",
    "    print(\"Train/Test/Validation Ratios:\", train_ratio,\n",
    "          \"/\", test_ratio, \"/\", valid_ratio)\n",
    "\n",
    "\n",
    "# Call the function with train_df, test_df, and valid_df\n",
    "print_train_test_valid_amounts(train_df, test_df, valid_df)\n",
    "\n",
    "# Show the amount in each class for the train data\n",
    "print(\"\\nAmount of Cards in Each Class - Train:\\n---------------------------------------\")\n",
    "train_class_amts = train_df['label'].value_counts()\n",
    "\n",
    "for ind, card, amount in zip(range(0, len(train_class_amts)), train_class_amts.index, train_class_amts):\n",
    "    print(f'{ind + 1:02}.', card, '\\t', amount)\n",
    "\n",
    "train_count = len(train_df)\n",
    "print('\\nTotal\\t\\t\\t', train_count)\n",
    "\n",
    "num_classes = len(train_df['label'].unique())\n",
    "print(f\"\\nThere are {num_classes} different card classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef100f7b",
   "metadata": {},
   "source": [
    "We now can see the train/test/validation split based on the number of images that are allocated to each split type.\n",
    "\n",
    "We want to view some examples of the cards and their respective labels to get an understanding of the different types of images that are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70b2fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def display_one_per_face_value(train_df):\n",
    "    \"\"\"\n",
    "    Displays one image per face value (ace, two, three, etc.) from train_df.\n",
    "    \"\"\"\n",
    "    # Define the face values we want to display (in order)\n",
    "    face_values = ['ace', 'two', 'three', 'four', 'five', 'six', 'seven', \n",
    "                   'eight', 'nine', 'ten', 'jack', 'queen', 'king']\n",
    "    \n",
    "    # Create a dictionary to store one example of each face value\n",
    "    face_value_examples = {}\n",
    "    \n",
    "    # Find one example for each face value\n",
    "    for face_value in face_values:\n",
    "        # Filter dataframe for rows containing this face value\n",
    "        matching_rows = train_df[train_df['label'].str.startswith(face_value)]\n",
    "        \n",
    "        # If we found matches, take the first one\n",
    "        if not matching_rows.empty:\n",
    "            face_value_examples[face_value] = matching_rows.iloc[0]['file_path']\n",
    "    \n",
    "    # Create a 5×3 grid (to fit all 13 cards with one empty space)\n",
    "    fig, axs = plt.subplots(5, 3, figsize=(15, 20))\n",
    "    axs = axs.flatten()  # Flatten for easier indexing\n",
    "    \n",
    "    # Display each card\n",
    "    for i, face_value in enumerate(face_values):\n",
    "        if face_value in face_value_examples:\n",
    "            try:\n",
    "                # Load and display the image\n",
    "                img_path = face_value_examples[face_value]\n",
    "                img = Image.open(img_path)\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].set_title(f\"{face_value.title()}\", fontsize=12)\n",
    "            except Exception as e:\n",
    "                print(f\"Error displaying {face_value}: {e}\")\n",
    "        \n",
    "        # Hide axes\n",
    "        axs[i].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(face_values), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print paths of displayed images (helpful for debugging)\n",
    "    print(\"\\nDisplayed images:\")\n",
    "    for face_value in face_values:\n",
    "        if face_value in face_value_examples:\n",
    "            print(f\"{face_value.title()}: {face_value_examples[face_value]}\")\n",
    "            \n",
    "display_one_per_face_value(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f2be5",
   "metadata": {},
   "source": [
    "Assign X_train, X_test, X_val, Y_train, Y_test, and Y_val to our train/test/validation split dataframes.\n",
    "\n",
    "As of right now, our data is all with file paths, so we need to make an array with the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Function to load images and convert them to arrays\n",
    "\n",
    "\n",
    "def load_images(file_paths, target_size=(224, 224)):\n",
    "    image_data = []\n",
    "    for file_path in file_paths:\n",
    "        # Load image and resize\n",
    "        img = load_img(file_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)  # Convert image to array\n",
    "        image_data.append(img_array)\n",
    "    return np.array(image_data)\n",
    "\n",
    "\n",
    "# Create image arrays for train, test, and validation sets\n",
    "X_train = load_images(train_df['file_path'].values)\n",
    "X_test = load_images(test_df['file_path'].values)\n",
    "X_val = load_images(valid_df['file_path'].values)\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "y_val = valid_df['label'].values\n",
    "\n",
    "# Print shapes of the arrays\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb8086",
   "metadata": {},
   "source": [
    "Given this information, we were able to extract the shape of `X_train`, `X_test`, `X_val`. These three variables hold the image data, and we can see the shape clearly where the images are 224x224x3. For `y_train`, `y_test`, and `y_val`, they hold the labels of each image which we will use for predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f1059",
   "metadata": {},
   "source": [
    "Our next step is to perform label encoding on `y_train`, `y_test`, and `y_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# create a data frame to store these encoded labels,\n",
    "# as we'll need to refer to the original class labels later\n",
    "\n",
    "encoded_dict = pd.DataFrame(\n",
    "    {'train_labels': y_train, 'train_encoded_labels': y_train_encoded})\n",
    "encoded_dict = encoded_dict.groupby(\n",
    "    'train_labels')['train_encoded_labels'].mean().reset_index()\n",
    "encoded_dict['train_encoded_labels'] = encoded_dict['train_encoded_labels'].astype(\n",
    "    'int')\n",
    "\n",
    "print(\"Encoded class labels and their corresponding class name:\")\n",
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3387e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
